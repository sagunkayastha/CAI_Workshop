{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagunkayastha/CAI_Workshop/blob/main/Workshop_s2/DL_intro2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/sagunkayastha/CAI_Workshop/main/Workshop_s2/utils/utils.py\n",
        "!wget -q https://raw.githubusercontent.com/sagunkayastha/CAI_Workshop/main/Workshop_s2/data/bmi_data.csv"
      ],
      "metadata": {
        "id": "BSPBHSJY17eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8KBR_SlxBVW"
      },
      "source": [
        "***A Single Neuron***\n",
        "\n",
        "!['Single_neuron'](https://raw.githubusercontent.com/sagunkayastha/CAI_Workshop/main/Workshop_s2/images/i1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwif_qHYxBVY"
      },
      "source": [
        "Predicting crop yield based on two inputs:\n",
        "- the amount of water provided to the crop (irrigation)  \n",
        "- the amount of fertilizer used.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd5nke8UxBVY"
      },
      "source": [
        "-----------------\n",
        "Both of these factors have optimal ranges, and too much or too little of either can negatively impact the crop yield.\n",
        "\n",
        "- Water (Irrigation): Essential for growth, but both too little and too much can reduce crop yield due to drought stress or waterlogging.\n",
        "\n",
        "- Fertilizer: Needed for nutrients; however, too little can stunt growth due to deficiency, and too much can harm yield through toxicity and environmental damage.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br8ClXrBxBVZ"
      },
      "source": [
        "----\n",
        "Lets define weights(importance) based on thier impact on crop yield"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygl6lxfvxBVZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "weights = [0.6, 0.2] ## my assumption is that importance of water is higher than fertilizer\n",
        "bias = 0.1\n",
        "# 0.6 is the weight of the first input, 0.3 is the weight of the second input\n",
        "# 0.6 gallons of water and 0.3 lb of fertilizer\n",
        "x = [0.5, 0.3]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8KwQdjoxBVa"
      },
      "source": [
        "Combining Inputs: In our model, these inputs are weighted based on their impact on crop yield, with a bias term included to account for other factors influencing yield (such as soil quality or pest levels).\n",
        "\n",
        "$$z = \\sum_{i=1}^{n} x_i w_i + b$$\n",
        "\n",
        "$$output = \\sigma(z)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGAPeNwNxBVa"
      },
      "source": [
        "The model calculates the predicted crop yield by balancing the effects of water and fertilizer.\n",
        "\n",
        "It recognizes the non-linear relationship: both inputs contribute positively to yield up to a point, but beyond their optimal ranges, the effect reverses and becomes negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwNJ6TjpxBVa"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "z = (x[0] * weights[0]) + (x[1] * weights[1]) + bias\n",
        "crop_yield = sigmoid(z)\n",
        "print(\"Crop yield for 0.6 gallons of water and 0.3 lb of fertilizer: \", crop_yield)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reRM7F1fxBVb"
      },
      "source": [
        "Learning from Feedback (Backpropagation):\n",
        "After the harvest, the actual yields are compared to the model's predictions.\n",
        "\n",
        "This feedback allows the model to adjust the weights of water and fertilizer inputs.\n",
        "\n",
        "If yields are lower than expected at extreme values of either input, the model learns to adjust the importance (weight) it assigns to staying within optimal ranges.\n",
        "\n",
        "lets suppose the actual yield was 0.81"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KcKKvobxBVd"
      },
      "outputs": [],
      "source": [
        "# MSE loss function\n",
        "def loss_function(predicted, real):\n",
        "    return (predicted - real) **2\n",
        "\n",
        "actual = 0.81\n",
        "loss = loss_function(crop_yield, actual)\n",
        "print(\"Error(Loss): \", loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR11CN42xBVd"
      },
      "source": [
        "![loss](https://www.researchgate.net/publication/329960546/figure/fig2/AS:865846410899458@1583445279578/Weight-update-by-gradient-descent-in-the-cost-function.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eckbt2GaxBVd"
      },
      "source": [
        "Adjusting Weights: **Adjusting Knobs** Through backpropagation, the model:\n",
        "\n",
        "- Increases the negative weight of water and fertilizer inputs as they move beyond their optimal ranges, reflecting the detrimental effects of both excessive and insufficient application.\n",
        "- Fine-tunes the bias and weights to better capture the complex, non-linear relationships between inputs and crop yield, aiming for the optimal use of resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNkDjYrwxBVe"
      },
      "outputs": [],
      "source": [
        "# Forward propagation\n",
        "\n",
        "def forward(x, weights, bias):\n",
        "    z = np.dot(x, weights) + bias\n",
        "    return sigmoid(z)\n",
        "\n",
        "# change weights\n",
        "# Try changing the weights and bias to see how they affect the error\n",
        "weights = [1.4, 0.4]\n",
        "bias = 0.1\n",
        "x = [0.6, 0.3]\n",
        "crop_yield = forward(x, weights, bias)\n",
        "loss = loss_function(crop_yield, actual)\n",
        "print(\"Error(Loss): \", loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNFze87cxBVe"
      },
      "source": [
        "\n",
        "Making Predictions: First, the network makes predictions based on its current settings (weights). Think of these weights like knobs that can be turned to change the network's behavior.\n",
        "\n",
        "Measuring Mistakes: After making predictions, the network looks at how far off it was from the correct answers. This difference is called the loss, and the network's goal is to make this as small as possible.\n",
        "\n",
        "Asking \"How Much?\": The network then asks, \"How much does each weight affect the loss?\" To find this out, it computes the partial derivatives of the loss function with respect to each weight. These partial derivatives are called gradients.\n",
        "\n",
        "Finding Direction: The gradients tell the network not just how much, but also in which direction to adjust each weight (knob) to reduce the mistakes. If a gradient is positive, reducing the weight decreases the loss, and if it's negative, increasing the weight does.\n",
        "\n",
        "Adjusting Knobs: Finally, the network slightly adjusts each knob (weight) in the direction indicated by the gradients to make better predictions next time. This step is repeated many times, and with each repetition, the network gets better at making predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CADQh7UxBVe"
      },
      "source": [
        "---------------\n",
        "\n",
        "--------------\n",
        "\n",
        "The gradients for updating the weights and bias are calculated using the chain rule as follows:\n",
        "\n",
        "**Dominoes** - first output then activation then summation(z)\n",
        "\n",
        "- Gradient with respect to weights:\n",
        "  $$dLoss/dWeights = dLoss/dOutput \\cdot dOutput/dZ \\cdot dZ/dWeights$$\n",
        "  \n",
        "- Gradient with respect to bias:\n",
        "  $$dLoss/dBias = dLoss/dOutput \\cdot dOutput/dZ \\cdot dZ/dBias$$\n",
        "\n",
        "\n",
        "Ignore calculus if you find it too complicated. But basically we are trying to find out how much affect does each weight (gallons of water and fertilizer) has on our final loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPcDLgDmxBVf"
      },
      "source": [
        "\n",
        "Using a learning rate eta we update the weights and bias as follows:\n",
        "\n",
        "$$w_i^{new} = w_i - \\eta \\cdot \\frac{\\partial L}{\\partial w_i}$$\n",
        "$$b^{new} = b - \\eta \\cdot \\frac{\\partial L}{\\partial b}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNBjSjITxBVf"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def backward(x, weights, bias, output, target, learning_rate):\n",
        "    \"\"\"Perform backpropagation and update the weights and bias.\"\"\"\n",
        "    # Compute the derivative of the loss with respect to output\n",
        "    dLoss_dOutput = -(target - output)  # we ignore the factor of 2 for simplicity\n",
        "\n",
        "    # Compute the derivative of the output with respect to z\n",
        "    dOutput_dZ = output * (1 - output)\n",
        "\n",
        "    # Compute the gradient of the loss with respect to weights\n",
        "    dLoss_dWeights = dLoss_dOutput * dOutput_dZ * x\n",
        "\n",
        "    # Compute the gradient of the loss with respect to bias\n",
        "    dLoss_dBias = dLoss_dOutput * dOutput_dZ\n",
        "\n",
        "\n",
        "    # Update the weights and bias\n",
        "    weights -= learning_rate * dLoss_dWeights\n",
        "    bias -= learning_rate * dLoss_dBias\n",
        "\n",
        "    return weights, bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnRBVwI8xBVf"
      },
      "source": [
        "Lets implement forward and backward pass together. This is called an **Iteration**(**Terminology**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANOYmzq_xBVf"
      },
      "outputs": [],
      "source": [
        "\n",
        "weights = np.array([0.6, 0.2])\n",
        "bias = np.array(0.1)\n",
        "x = np.array([0.6, 0.3])\n",
        "\n",
        "\n",
        "crop_yield = forward(x, weights, bias)\n",
        "loss = loss_function(crop_yield, actual)\n",
        "print(\"Error(Loss): \", loss)\n",
        "\n",
        "weights, bias = backward(x, weights, bias, crop_yield, actual, 0.1)\n",
        "print(\"Updated weights: \", weights, \"Updated bias: \", bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obSGmcDexBVg"
      },
      "source": [
        "We would perform this iteratively for all the samples in our dataset. We can update the weights for each example, for a batch of example or for whole dataset.\n",
        "\n",
        "\n",
        " Stochastic Gradient Descent, Batch Gradient descent (**Terminology**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6iBA1rexBVg"
      },
      "outputs": [],
      "source": [
        "# since we have only one data point, we can update the weights and bias directly.\n",
        "# This is basically Batch Gradient Descent where we use all the data points to update the weights and bias. and our batch size is 1\n",
        "\n",
        "\n",
        "weights = np.array([0.6, 0.2])\n",
        "\n",
        "bias = np.array(0.1)\n",
        "x = np.array([0.6, 0.3])\n",
        "\n",
        "initial_wb = [weights.copy(), bias.copy()]\n",
        "for epoch in range(100):\n",
        "    crop_yield = forward(x, weights, bias)\n",
        "    loss = loss_function(crop_yield, actual)\n",
        "    weights, bias = backward(x, weights, bias, crop_yield, actual, learning_rate=0.1)\n",
        "    print(\"Error(Loss) epoch: \", loss)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unsMjdpWxBVg"
      },
      "outputs": [],
      "source": [
        "print(\"Initial weights and bias: \", initial_wb[0], initial_wb[1])\n",
        "print(\"Updated weights and bias: \", weights, bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3Z9btQUxBVg"
      },
      "source": [
        "# Lets try a generated dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjwP2jR4xBVh"
      },
      "outputs": [],
      "source": [
        "from utils import generate_data, plot_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytS8_FJ8xBVh"
      },
      "outputs": [],
      "source": [
        "x1, x2, y = generate_data(1000)\n",
        "fig = plot_data(x1, x2, y)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8YQ1mCTxBVh"
      },
      "source": [
        "***Normalization***(Terminology) is a step in preparing data for machine learning that makes all the data similar in scale. This is important because:\n",
        "\n",
        "- Helps Learn Faster: It makes the machine learning model learn and make predictions faster.\n",
        "- Fair Treatment: Ensures every piece of data is treated equally by the model, so no single type of data overpowers others.\n",
        "- Better Predictions: Leads to more accurate and stable predictions from the model.\n",
        "- Works Well with Many Models: Some machine learning models need data to be normalized to work correctly.\n",
        "- Avoids Problems: Prevents issues that can happen when data is in very different scales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeBv6HTTxBVh"
      },
      "outputs": [],
      "source": [
        "# normalize the data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X1 = scaler.fit_transform(x1.reshape(-1, 1)).flatten()\n",
        "X2 = scaler.fit_transform(x2.reshape(-1, 1)).flatten()\n",
        "\n",
        "X = np.array([X1, X2]).T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xoDlFO0xBVi"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3boeWfrxBVi"
      },
      "outputs": [],
      "source": [
        "# Lets manually initialize the weights and bias\n",
        "weights = np.array([-0.2, 0.4])\n",
        "bias = np.array([0.4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zM_wAyVFxBVi"
      },
      "outputs": [],
      "source": [
        "# Start with a single example\n",
        "x_input = X[5]\n",
        "actual = y[5]\n",
        "\n",
        "output = forward(x_input, weights, bias)\n",
        "loss = loss_function(output, actual)\n",
        "print(\"Error(Loss): \", loss)\n",
        "weights, bias = backward(x_input, weights, bias, output, actual, learning_rate=0.1)\n",
        "print(\"Updated weights and bias: \", weights, bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziK0rPEBxBVi"
      },
      "source": [
        "Single Epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKDBNr3vxBVj"
      },
      "outputs": [],
      "source": [
        "epoch_loss = 0\n",
        "for iteration, (x_input,actual) in enumerate(zip(X, y)):\n",
        "    output = forward(x_input, weights, bias)\n",
        "    loss = loss_function(output, actual)\n",
        "    weights, bias = backward(x_input, weights, bias, output, actual, learning_rate=0.1)\n",
        "\n",
        "    # print(\"Previous output:\", output, \"Previous loss:\", loss)\n",
        "    # print(\"Updated output:\", updated_output, \"Updated loss:\", updated_loss)\n",
        "    epoch_loss += loss\n",
        "\n",
        "epoch_loss = epoch_loss / len(X)\n",
        "print(\"First Epoch loss:\", epoch_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZgrbtI8xBVj"
      },
      "source": [
        "Now for 100 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1yv6yG6xBVj"
      },
      "outputs": [],
      "source": [
        "weights = np.array([-0.2, 0.4])\n",
        "bias = np.array([0.4])\n",
        "epoch_losses = []\n",
        "for epoch in range(100): # This is the number of times we iterate through the entire dataset\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for iteration, (x_input, actual) in enumerate(zip(X, y)):\n",
        "        output = forward(x_input, weights, bias)\n",
        "        loss = loss_function(output, actual)\n",
        "        weights, bias = backward(x_input, weights, bias, output, actual, learning_rate=0.01)\n",
        "\n",
        "        # print(\"Previous output:\", output, \"Previous loss:\", loss)\n",
        "        # print(\"Updated output:\", updated_output, \"Updated loss:\", updated_loss)\n",
        "        epoch_loss += loss\n",
        "\n",
        "    epoch_loss = epoch_loss / len(X)\n",
        "    epoch_losses.append(epoch_loss)\n",
        "    print(f\"Epoch loss: {epoch}\", epoch_loss[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hOGaA7exBVj"
      },
      "source": [
        "Same data with tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itqMXaEyxBVk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers.experimental import SGD\n",
        "\n",
        "\n",
        "np.random.seed(402)\n",
        "tf.random.set_seed(42)\n",
        "weights = np.array([-0.2, 0.4])\n",
        "bias = np.array([0.4])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=402)\n",
        "\n",
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid', input_shape=(2,),\n",
        "                          kernel_initializer=tf.keras.initializers.Constant(weights),\n",
        "                          bias_initializer=tf.keras.initializers.Constant(bias))\n",
        "])\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='SGD', loss='mse')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSR52s8fxBVk"
      },
      "source": [
        "#### Number of Parameters\n",
        "- Resnet50 -> 25M\n",
        "\n",
        "- gpt-4 -> 1.76 trillion parameters\n",
        "\n",
        "- llama2 -> 7B, 13B, 70B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZH972QXxBVk"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=1, validation_data=(X_test, y_test), validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEXwAqelxBVk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(units=6, activation='relu', input_shape=(2,),\n",
        "                          ),\n",
        "    tf.keras.layers.Dense(units=3, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=1)\n",
        "\n",
        "])\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try changing the parameters and hyperparameters"
      ],
      "metadata": {
        "id": "7LNlW23h2TFt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuMvAg3JxBVy"
      },
      "source": [
        "##### Machine Learning Model vs Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Z6mo9WwxBVz"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error for Neural Network :\", mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lf_hHe6TxBVz"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rf = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
        "rf.fit(X_train, y_train.ravel())\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error for Random Forest:\", mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxTMyaAvxBVz"
      },
      "source": [
        "### Parameters vs Hyperparameters\n",
        "\n",
        "- Definition: Parameters are learned from data; hyperparameters are set before training.\n",
        "- Role: Parameters make predictions; hyperparameters guide the learning process.\n",
        "- Adjustment: Parameters adjust automatically; hyperparameters are chosen manually (or can use searched using algorithms).\n",
        "- Examples: Parameters are weights/biases; hyperparameters include learning rate, epochs.\n",
        "- Optimization: Parameters optimized during training; hyperparameters through testing various settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujwysJ7UxBV0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('bmi_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "_At1M7Jj2crs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a classification task, here we are trying to predict BMI based on Gender, Height and Weight"
      ],
      "metadata": {
        "id": "hEy-Z1LP2e76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessing\n",
        "\n",
        "- Convert Gender to numeric categorical variable\n",
        "- Normalize the input data for better neural network performance.\n",
        "- Split the data into training and testing sets."
      ],
      "metadata": {
        "id": "6X-DcRNK2gBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Gender'] = df['Gender'].map({'Male': 0, 'Female': 1})\n",
        "df.head()"
      ],
      "metadata": {
        "id": "1VpK4Rwc2dp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X = df[['Gender', 'Height', 'Weight']].values\n",
        "\n",
        "# Normalize X\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bNkrdj2a2g2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For classification problem we have to change few things.\n",
        "- Input shape\n",
        "- Activation in the output layer\n",
        "- Loss Function\n",
        "\n",
        "You can have 6 outputs(one hot encoded) with softmax or 1 output(0 to 6) with sigmoid. The loss function will depend on what you choose for the ouput layer.\n",
        "\n",
        "In this example we are using one hot encoded y, softmax with categorical_crossentropy"
      ],
      "metadata": {
        "id": "Nmb1wl9f2i6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "0 -> [1, 0, 0, 0, 0, 0]\n",
        "\n",
        "1 -> [0, 1, 0, 0, 0, 0]\n",
        "\n",
        "2 -> [0, 0, 1, 0, 0, 0]\n",
        "\n",
        "and so on"
      ],
      "metadata": {
        "id": "kUnCjWUO2kOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "Y = to_categorical(df['Index'].values)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_normalized, Y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "NdX5jjyJ2hwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For classification p\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(units=16, activation='relu', input_shape=(3,),\n",
        "                          ),\n",
        "    tf.keras.layers.Dense(units=8, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=6, activation='softmax'),\n",
        "\n",
        "])\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # for classification problems, we use categorical_crossentropy\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "5RqFn64W2lda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Softmax activation\n",
        "![Softmax](https://docs-assets.developer.apple.com/published/c2185dfdcf/0ab139bc-3ff6-49d2-8b36-dcc98ef31102.png)"
      ],
      "metadata": {
        "id": "te3pXOG42nyl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DvLLByur2mQq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}