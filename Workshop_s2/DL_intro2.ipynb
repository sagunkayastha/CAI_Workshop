{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagunkayastha/CAI_Workshop/blob/main/Workshop_s2/DL_intro2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/sagunkayastha/CAI_Workshop/main/Workshop_s2/utils/utils.py"
      ],
      "metadata": {
        "id": "_S2HK8YEMPf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eum7bI9pJn-G"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie2ZIHeuJn-H"
      },
      "source": [
        "***A Single Neuron***\n",
        "\n",
        "!['Single_neuron'](https://raw.githubusercontent.com/sagunkayastha/CAI_Workshop/main/Workshop_s2/images/i1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VF2JV-BJn-J"
      },
      "source": [
        "$$z = \\sum_{i=1}^{n} x_i w_i + b$$\n",
        "\n",
        "$$output = \\sigma(z)$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiAPsKOcJn-J"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def single_neuron(x, weights, bias):\n",
        "    # Try looping through each element in the input vector and multiplying it by the corresponding weight\n",
        "    z = np.dot(x, weights) + bias\n",
        "    output = sigmoid(z)\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "x = np.array([0.5, 0.3])  # Input vector\n",
        "weights = np.array([0.4, -0.2])  # Weights for each input\n",
        "bias = np.array([0.1])  # Bias\n",
        "\n",
        "output = single_neuron(x, weights, bias)\n",
        "print(\"x shape:\", x.shape, \"weights shape:\", weights.shape, \"bias shape:\", bias.shape)\n",
        "print(\"Output of the single neuron:\", output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uib-onsdJn-K"
      },
      "source": [
        "$$MSE=(realâˆ’output)^2$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlf7U3VpJn-L"
      },
      "outputs": [],
      "source": [
        "\n",
        "def loss_function(predicted, real):\n",
        "    return (predicted - real) ** 2\n",
        "\n",
        "loss = loss_function(output, 0.7)\n",
        "print(\"Loss:\", loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGFen-gnJn-L"
      },
      "source": [
        "The gradients for updating the weights and bias are calculated using the chain rule as follows:\n",
        "\n",
        "- Gradient with respect to weights:\n",
        "  $$dLoss/dWeights = dLoss/dOutput \\cdot dOutput/dZ \\cdot dZ/dWeights$$\n",
        "  \n",
        "- Gradient with respect to bias:\n",
        "  $$dLoss/dBias = dLoss/dOutput \\cdot dOutput/dZ \\cdot dZ/dBias$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q5m2acrJn-L"
      },
      "source": [
        "---------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l62i9JYDJn-L"
      },
      "source": [
        "\n",
        "$$\\frac{dLoss}{dOutput} = -2 \\times (real - output)$$\n",
        "\n",
        "\n",
        "\n",
        "$$\\frac{dOutput}{dZ} = \\sigma(z) \\cdot (1 - \\sigma(z))  = output \\cdot (1 - output)$$\n",
        "\n",
        "\n",
        "$$\\frac{dZ}{dW_i} = x_i$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GquKQ6B-Jn-M"
      },
      "source": [
        "---------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7aSGjOYJn-M"
      },
      "source": [
        "\n",
        "Using a learning rate eta update the weights and bias as follows:\n",
        "\n",
        "$$w_i^{new} = w_i - \\eta \\cdot \\frac{\\partial L}{\\partial w_i}$$\n",
        "$$b^{new} = b - \\eta \\cdot \\frac{\\partial L}{\\partial b}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaDIqdPrJn-M"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def update_weights_and_bias(x, weights, bias, output, target, learning_rate):\n",
        "    \"\"\"Perform backpropagation and update the weights and bias.\"\"\"\n",
        "    # Compute the derivative of the loss with respect to output\n",
        "    dLoss_dOutput = -(target - output)  # we ignore the factor of 2 for simplicity\n",
        "\n",
        "    # Compute the derivative of the output with respect to z\n",
        "    dOutput_dZ = output * (1 - output)\n",
        "\n",
        "    # Compute the gradient of the loss with respect to weights\n",
        "    dLoss_dWeights = dLoss_dOutput * dOutput_dZ * x\n",
        "\n",
        "    # Compute the gradient of the loss with respect to bias\n",
        "    dLoss_dBias = dLoss_dOutput * dOutput_dZ\n",
        "\n",
        "\n",
        "    # Update the weights and bias\n",
        "    weights -= learning_rate * dLoss_dWeights\n",
        "    bias -= learning_rate * dLoss_dBias\n",
        "\n",
        "    return weights, bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7IQnMZcJn-N"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqTRv-A_Jn-N"
      },
      "outputs": [],
      "source": [
        "updated_weights, updated_bias = update_weights_and_bias(x, weights, bias, output, 0.7, 0.1)\n",
        "\n",
        "updated_output = single_neuron(x, updated_weights, updated_bias)\n",
        "updated_loss = loss_function(updated_output, 0.7)\n",
        "\n",
        "\n",
        "print(\"Previous output:\", output, \"Previous loss:\", loss)\n",
        "print(\"Updated output:\", updated_output, \"Updated loss:\", updated_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2fs7OMUJn-N"
      },
      "outputs": [],
      "source": [
        "print(\"Previous weights:\", weights, \"Previous bias:\", bias)\n",
        "print(\"Updated weights:\", updated_weights, \"Updated bias:\", updated_bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bth2RZ1DJn-N"
      },
      "source": [
        "!['Two2_neuron'](https://raw.githubusercontent.com/sagunkayastha/CAI_Workshop/main/Workshop_s2/images/i2.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrQtTzVjJn-O"
      },
      "outputs": [],
      "source": [
        "def two_layers(x, weights1, weights2, biases1, biases2):\n",
        "    a12_a22 = sigmoid(np.dot(x, weights1) + biases1)\n",
        "    ouptut = sigmoid(np.dot(a12_a22, weights2) + biases2)\n",
        "\n",
        "    return output\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "x = np.array([0.5, 0.3])\n",
        "\n",
        "# Weights for each neuron, each column represents a single neuron\n",
        "weights1 = np.array([[0.4, -0.2],\n",
        "                    [0.1, 0.6], ])\n",
        "weights2 = np.array([0.1, -0.2])  # Weights for the output layer\n",
        "# Biases for each neuron\n",
        "biases1 = np.array([0.1, -0.2])\n",
        "biases2 = np.array([0.2])\n",
        "\n",
        "output = two_layers(x, weights, weights2, biases1,  biases2)\n",
        "\n",
        "print(\"x shape, weight1 shape\", x.shape, weights1.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_wfC7M2Jn-O"
      },
      "source": [
        "----------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSylHsmfJn-O"
      },
      "source": [
        "##### Trying similar model with generated dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsTWaGI5Jn-O"
      },
      "outputs": [],
      "source": [
        "from utils import generate_data, plot_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWg_P-3gJn-O"
      },
      "outputs": [],
      "source": [
        "x1, x2, y = generate_data(1000)\n",
        "fig = plot_data(x1, x2, y)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIcXai0CJn-O"
      },
      "source": [
        "***Normalization*** is a step in preparing data for machine learning that makes all the data similar in scale. This is important because:\n",
        "\n",
        "- Helps Learn Faster: It makes the machine learning model learn and make predictions faster.\n",
        "- Fair Treatment: Ensures every piece of data is treated equally by the model, so no single type of data overpowers others.\n",
        "- Better Predictions: Leads to more accurate and stable predictions from the model.\n",
        "- Works Well with Many Models: Some machine learning models need data to be normalized to work correctly.\n",
        "- Avoids Problems: Prevents issues that can happen when data is in very different scales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1CF-BUQJn-P"
      },
      "outputs": [],
      "source": [
        "# normalize the data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X1 = scaler.fit_transform(x1.reshape(-1, 1)).flatten()\n",
        "X2 = scaler.fit_transform(x2.reshape(-1, 1)).flatten()\n",
        "\n",
        "X = np.array([X1, X2]).T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J77pQY3nJn-P"
      },
      "outputs": [],
      "source": [
        "#  initialize the weights and bias\n",
        "weights = np.random.rand(2)\n",
        "bias = np.random.rand(1)\n",
        "\n",
        "# take a single example\n",
        "single_x = X[50]\n",
        "single_y = y[50]\n",
        "\n",
        "output = single_neuron( single_x, weights, bias)\n",
        "loss = loss_function(output, single_y)\n",
        "print(\"Output:\", output, \"Loss:\", loss)\n",
        "updated_weights, updated_bias = update_weights_and_bias(single_x, weights, bias, output, single_y, 0.1)\n",
        "\n",
        "updated_output = single_neuron(single_x, updated_weights, updated_bias)\n",
        "updated_loss = loss_function(updated_output, single_y)\n",
        "print(\"Previous output:\", output, \"Previous loss:\", loss)\n",
        "print(\"Updated output:\", updated_output, \"Updated loss:\", updated_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUEe6lisJn-P"
      },
      "outputs": [],
      "source": [
        "epoch_loss = 0\n",
        "for single_x, single_y in zip(X, y):\n",
        "    output = single_neuron( single_x, weights, bias)\n",
        "    loss = loss_function(output, single_y)\n",
        "    weights, bias = update_weights_and_bias(single_x, weights, bias, output, single_y, 0.1)\n",
        "\n",
        "    # print(\"Previous output:\", output, \"Previous loss:\", loss)\n",
        "    # print(\"Updated output:\", updated_output, \"Updated loss:\", updated_loss)\n",
        "    epoch_loss += loss\n",
        "\n",
        "epoch_loss = epoch_loss / len(X)\n",
        "print(\"First Epoch loss:\", epoch_loss)\n",
        "\n",
        "epoch_loss = 0\n",
        "for single_x, single_y in zip(X, y):\n",
        "    output = single_neuron( single_x, weights, bias)\n",
        "    loss = loss_function(output, single_y)\n",
        "    weights, bias = update_weights_and_bias(single_x, weights, bias, output, single_y, 0.1)\n",
        "\n",
        "    # print(\"Previous output:\", output, \"Previous loss:\", loss)\n",
        "    # print(\"Updated output:\", updated_output, \"Updated loss:\", updated_loss)\n",
        "    epoch_loss += loss\n",
        "\n",
        "epoch_loss = epoch_loss / len(X)\n",
        "print(\"Second Epoch loss:\", epoch_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yAygFbwJn-P"
      },
      "outputs": [],
      "source": [
        "\n",
        "np.random.seed(402)\n",
        "weights = np.random.rand(2)\n",
        "bias = np.random.rand(1)\n",
        "\n",
        "epoch_losses = []\n",
        "for epoch in range(100): # This is the number of times we iterate through the entire dataset\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for single_x, single_y in zip(X, y): ## This is iterating through the entire dataset\n",
        "        output = single_neuron( single_x, weights, bias)\n",
        "        loss = loss_function(output, single_y)\n",
        "        weights, bias = update_weights_and_bias(single_x, weights, bias, output, single_y, 0.01)\n",
        "\n",
        "        # print(\"Previous output:\", output, \"Previous loss:\", loss)\n",
        "        # print(\"Updated output:\", updated_output, \"Updated loss:\", updated_loss)\n",
        "        epoch_loss += loss\n",
        "\n",
        "    epoch_loss = epoch_loss / len(X)\n",
        "    epoch_losses.append(epoch_loss)\n",
        "    print(f\"Epoch loss: {epoch}\", epoch_loss[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYIsDSGHJn-Q"
      },
      "source": [
        "##### Tensorflow model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dKNq0EFJn-Q"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers.experimental import SGD\n",
        "\n",
        "\n",
        "np.random.seed(402)\n",
        "tf.random.set_seed(42)\n",
        "weights = np.random.rand(2)\n",
        "bias = np.random.rand(1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=402)\n",
        "\n",
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid', input_shape=(2,),\n",
        "                          kernel_initializer=tf.keras.initializers.Constant(weights),\n",
        "                          bias_initializer=tf.keras.initializers.Constant(bias))\n",
        "])\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tIoulJuJn-Q"
      },
      "source": [
        "#### Number of Parameters\n",
        "resnet50 25M\n",
        "\n",
        "gpt-4 1.76 trillion parameters\n",
        "\n",
        "llama2 7B, 13B, 70B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNubPQ8lJn-Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "model.compile(optimizer='SGD', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=1, validation_data=(X_test, y_test), validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ml8H_YaLJn-Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(units=6, activation='relu', input_shape=(2,),\n",
        "                          ),\n",
        "    tf.keras.layers.Dense(units=3, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=1)\n",
        "\n",
        "])\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjTY_h8QJn-R"
      },
      "source": [
        "##### Machine Learning Model vs Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdXbcAoDJn-R"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error for Neural Network :\", mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIofgK19Jn-R"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rf = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
        "rf.fit(X_train, y_train.ravel())\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error for Random Forest:\", mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM9ElmiiJn-R"
      },
      "source": [
        "### Parameters vs Hyperparameters\n",
        "\n",
        "- Definition: Parameters are learned from data; hyperparameters are set before training.\n",
        "- Role: Parameters make predictions; hyperparameters guide the learning process.\n",
        "- Adjustment: Parameters adjust automatically; hyperparameters are chosen manually (or can use searched using algorithms).\n",
        "- Examples: Parameters are weights/biases; hyperparameters include learning rate, epochs.\n",
        "- Optimization: Parameters optimized during training; hyperparameters through testing various settings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7VKp1G1Jn-R"
      },
      "source": [
        "#### Try creating NN for bmi_data in data folder"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}